{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "virgin-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, LineSearches, Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-shade",
   "metadata": {},
   "source": [
    "# ME 617 HW 1\n",
    "\n",
    "_Note: This source code was implemented in Julia. This notebook requires a Julia 1.7.x kernel as well as an environment that includes the necessary dependencies. See the [repo](https://github.com/camirmas/DesignAutomation) for full implementations, testing, and dependency information. All relevant code has been copied into this notebook, so no importing of individual modules is necessary._\n",
    "\n",
    "All problems involve the following objective function:\n",
    "\n",
    "$f(x_1, x_2) = (2x_2-3x_1^2)^4+(5-2x_1)^2$\n",
    "\n",
    "And use the starting point:\n",
    "\n",
    "$(x_1, x_2) = (0, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cloudy-mainland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn(x) = (2*x[2]-3*x[1]^2)^4 + (5-2*x[1])^2\n",
    "x0 = [0., 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-privacy",
   "metadata": {},
   "source": [
    "For reference, let's see what the answer to this problem is according to a popular optimization library, [Optim](https://github.com/JuliaNLSolvers/Optim.jl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "welsh-driver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Final objective value:     1.065787e-09\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    55\n",
       "    f(x) calls:    107\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = optimize(fn, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "pending-toilet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 2.5000162227450113\n",
       " 9.37607248831831"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-degree",
   "metadata": {},
   "source": [
    "## 1. CCS\n",
    "\n",
    "Description: \n",
    "\n",
    "Solve the problem with Cyclic Coordinate Search for 12 iterations. Solve (a) with acceleration steps and compare to (b) without the acceleration step. For (a), you would search $e_1$, then $e_2$, then the acceleration, four times. For (b), you would search $e_1$, $e_2$, for six times). Recall that this method requires a line search like Golden Section...\n",
    "\n",
    "#### a)\n",
    "\n",
    "This version of the algorithm includes acceleration, and was initially implemented using convergence criteria (commented out because otherwise it converges before reaching 12 iterations). The fully tested algorithm can be found in the [repo](https://github.com/camirmas/DesignAutomation/blob/main/src/ccs.jl) with accompanying [tests](https://github.com/camirmas/DesignAutomation/blob/main/test/test_ccs.jl). Note that this function actually reaches convergence well before 12 iterations (it appears to be done after 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "literary-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Response\n",
    "    minimizer\n",
    "    minimum::AbstractFloat\n",
    "    iterations::Int\n",
    "    converged::Bool\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "graphic-productivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ccs_a"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements the Cyclic Coordinate Search optimization method. Uses a Hager-Zhang\n",
    "linesearch to determine step size.\n",
    "\"\"\"\n",
    "function ccs_a(fn, x0; ϵ=.001, max_iter=100)\n",
    "    algo_hz = Newton(linesearch = HagerZhang())\n",
    "    i = 1\n",
    "    k = 0\n",
    "    n = length(x0)\n",
    "    results = [(x0, fn(x0))]\n",
    "    e_hat = Matrix(1I, n, n)\n",
    "\n",
    "    while k < max_iter\n",
    "        x, _ = results[end]\n",
    "\n",
    "        if i == n + 1\n",
    "            d_k = x - results[k-n+1][1]\n",
    "            i = 1\n",
    "        else\n",
    "            d_k = e_hat[:, i]\n",
    "        end\n",
    "\n",
    "        α_fn(α) = fn(x + α .* d_k)\n",
    "        res = Optim.optimize(α_fn, x, method=algo_hz)\n",
    "\n",
    "        α = res.minimizer\n",
    "        x_new = x + α .* d_k\n",
    "        f_new = fn(x_new)\n",
    "        push!(results, (x_new, f_new))\n",
    "\n",
    "        k += 1\n",
    "        i += 1\n",
    "\n",
    "#         if norm(x_new-x) < ϵ\n",
    "#             return Response(x_new, f_new, k, true)\n",
    "#         end\n",
    "    end\n",
    "\n",
    "    println(\"Maximum iterations reached.\")\n",
    "    x, fx = results[end]\n",
    "    return Response(x, fx, k, false)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "concrete-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response([2.500000005615144, 9.374999993584302], 1.2611936431358006e-16, 12, false)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ccs_a(fn, x0; max_iter=12)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "recognized-connecticut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 2.500000005615144\n",
       " 9.374999993584302"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "experimental-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2611936431358006e-16"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-cheat",
   "metadata": {},
   "source": [
    "#### b)\n",
    "\n",
    "This function has been modified to remove the acceleration step. Note that this function does not actually converge in 12 iterations. This suggests that acceleration strongly influences the speed at which CCS converges, at least for objective functions resembling this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "saving-coating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ccs_b"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements the Cyclic Coordinate Search optimization method. Uses a Hager-Zhang\n",
    "linesearch to determine step size.\n",
    "\"\"\"\n",
    "function ccs_b(fn, x0; ϵ=.001, max_iter=100)\n",
    "    algo_hz = Newton(linesearch = HagerZhang())\n",
    "    i = 1\n",
    "    k = 0\n",
    "    n = length(x0)\n",
    "    results = [(x0, fn(x0))]\n",
    "    e_hat = Matrix(1I, n, n)\n",
    "\n",
    "    while k < max_iter\n",
    "        x, _ = results[end]\n",
    "\n",
    "        if i > n\n",
    "            i = 1\n",
    "        end\n",
    "        \n",
    "        d_k = e_hat[:, i]\n",
    "\n",
    "        α_fn(α) = fn(x + α .* d_k)\n",
    "        res = Optim.optimize(α_fn, x, method=algo_hz)\n",
    "\n",
    "        α = res.minimizer\n",
    "        x_new = x + α .* d_k\n",
    "        f_new = fn(x_new)\n",
    "        push!(results, (x_new, f_new))\n",
    "\n",
    "        k += 1\n",
    "        i += 1\n",
    "\n",
    "#         if norm(x_new-x) < ϵ\n",
    "#             return Response(x_new, f_new, k, true)\n",
    "#         end\n",
    "    end\n",
    "\n",
    "    println(\"Maximum iterations reached.\")\n",
    "    x, fx = results[end]\n",
    "    return Response(x, fx, k, false)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "urban-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response([1.2774343915191781, 2.4477579276687598], 5.978666668160328, 12, false)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ccs_b(fn, x0; max_iter=12)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "patient-assumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 1.2774343915191781\n",
       " 2.4477579276687598"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "taken-baker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.978666668160328"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-paint",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "Overall, these two examples emphasize the strength of acceleration steps, at least in this scenario. In comparing this implementation of CCS against other algorithms such as Hooke & Jeeves, it is useful to remember that the number of iterations performed in the 1-D line search is not encapsulated in the result. Even though the accelerated CCS completed in 4 \"iterations,\" the total number of function calls is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-dressing",
   "metadata": {},
   "source": [
    "## 2. Hooke & Jeeves\n",
    "\n",
    "Description: Solve the problem using Hooke and Jeeves for 24 function evaluations. Set r = 0.5 and h0 = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-enzyme",
   "metadata": {},
   "source": [
    "This algorithm was initially implemented using convergence criteria, which don't appear to be met for the number of iterations requested for this problem. The fully tested algorithm can be found in the [repo](https://github.com/camirmas/DesignAutomation/blob/main/src/hooke_jeeves.jl) with accompanying [tests](https://github.com/camirmas/DesignAutomation/blob/main/test/test_hooke_jeeves.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "applicable-momentum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hooke_jeeves"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements the Hooke & Jeeves direct search method.\n",
    "\n",
    "Arguments:\n",
    "    fn: objective function\n",
    "    x0: starting point\n",
    "    h0: starting step size\n",
    "    h_min: minimum step size\n",
    "    r: reduction factor for step size\n",
    "\n",
    "Returns:\n",
    "    `Response` struct if successful, else `nothing`\n",
    "\"\"\"\n",
    "function hooke_jeeves(fn, x0; h0=1/10, h_min=.01, r=1/2, max_iter=1000)\n",
    "    x = copy(x0)\n",
    "    n = length(x)\n",
    "    e_hat = Matrix(1I, n, n)\n",
    "    k = 0\n",
    "    h = h0\n",
    "    x_b = x\n",
    "    fx = fn(x)\n",
    "    results = [(x, fx)]\n",
    "\n",
    "    while k < max_iter\n",
    "        for i = 1:n\n",
    "            x, fx = results[end]\n",
    "            # try searching in each linearly independent direction\n",
    "            x_new = x + h * e_hat[:, i]\n",
    "            f_new = fn(x_new)\n",
    "\n",
    "            if f_new <= fx\n",
    "                k += 1\n",
    "                push!(results, (x_new, f_new))\n",
    "            else\n",
    "                x_new = x - h * e_hat[:, i]\n",
    "                f_new = fn(x_new)\n",
    "\n",
    "                if f_new <= fx\n",
    "                    k += 1\n",
    "                    push!(results, (x_new, f_new))\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        x, fx = results[end]\n",
    "        # if the base hasn't changed, reduce step size\n",
    "        if x_b == x\n",
    "            h *= r\n",
    "\n",
    "            # convergence reached\n",
    "            if h < h_min\n",
    "                return Response(x, fx, k, true)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # perform acceleration step\n",
    "        x_new = 2x - x_b\n",
    "        x_b = x\n",
    "        f_new = fn(x_new)\n",
    "\n",
    "        if f_new <= fx\n",
    "            k += 1\n",
    "            push!(results, (x_new, f_new))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"Maximum iterations reached.\")\n",
    "    x, fx = results[end]\n",
    "    return Response(x, fx, k, false)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "measured-norway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum iterations reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response([1.5, 3.25], 4.00390625, 24, false)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = hooke_jeeves(fn, x0; r=.5, h0=.5, max_iter=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-leather",
   "metadata": {},
   "source": [
    "This is not a very promising result after 24 iterations, given that we know the true answer. For reference, if we let the algorithm run to convergence (`r=.5`, `h0=.1`, `hmin=.01`), we get the following results, which appear consistent with the true answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "conceptual-midnight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response([2.4999999999999996, 9.37500000000003], 7.888609052210118e-31, 99, true)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = hooke_jeeves(fn, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-gallery",
   "metadata": {},
   "source": [
    "While the default knobs for this function result in convergence after 99 iterations, it is possible that an improved choice of knob values would yield the same result in less iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-burner",
   "metadata": {},
   "source": [
    "## 3. Discussion\n",
    "\n",
    "Description: Discuss differences in 1 & 2 using the criteria presented in ME517 (see slide 4 of Section5 slides). You can cite evidence from your results above, from challenges in writing the code, but also extrapolate to challenges in different problems, and iteration limits.\n",
    "\n",
    "#### Robustness\n",
    "\n",
    "Similar to the discussion about ease of use, CCS appears as robust as, if not more robust than, H&J, partially due to the fact that, at least in this implementation, H&J depends on the user choosing acceptable values for step sizes and reduction for the given sample space. A potential pitfall of this approach would involve the algorithm missing a minimum point (e.g. the step size is too large), or, as discussed in the textbook, taking too many small steps (see Efficiency). CCS, on the other hand, uses a robust, but potentially less efficient line search to determine step size at each iteration.\n",
    "\n",
    "#### Generality\n",
    "\n",
    "Neither algorithm imposes any particular restrictions on the objective function in a way that might cause issues. Both are intended to search the space as-is, as opposed to methods that attempt impose penalty functions, for example (though it should be noted that neither implementation is designed to handle constraints).\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "Both methods appear able to achieve precise and accurate results for minimum points for the types of problems investigated thus far in the course (e.g. Rosenbrock's banana). See [tests](https://github.com/camirmas/DesignAutomation/tree/main/test) for more examples of these algorithms performing against additional objective functions.\n",
    "\n",
    "#### Ease of use\n",
    "\n",
    "While both algorithms emphasize a simple, intuitive approach to optimization, and are relatively straightforward to implement, H&J appears more difficult for an end user, primarily because it has more tunable coefficients than Cyclic Coordinate Search. With H&J, small changes to `r`, `h0`, and `hmin` may have large influences on the algorithm's convergence, and may in fact cause problems with convergence (reducing too slowly or too quickly and/or defining step sizes that are too big or too small).\n",
    "\n",
    "#### Efficiency\n",
    "\n",
    "Comparing efficiency between CCS and H&J requires a number of considerations. Primarily, CCS and H&J are measuring \"iterations\" differently, namely that the 1-D linesearch in CCS is not included in the iteration counter, though it performs a non-negligible number of iterations of its own. As such, iteration limits for each algorithm should be set and considered differently. Additionally, as discussed in class, in the textbook, and in this assignment, H&J's performance is highly dependent on the user-provided knob settings. Poor settings may cause an unnecessarily high number of iterations and/or poor results, while informed ones may lead to a more efficient solution than that of CCS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
